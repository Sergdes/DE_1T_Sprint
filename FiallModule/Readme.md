# Финальное задание

В качестве финального задания нужно было выбрать и выполнить одно из 6 заданий (проектов). Ввиду насыщенности курса и информации, сложно было определится проектом. На реальной работе, что мне дали, то я и сделал бы.
Я выбрал первое задание, т.к. мне показалось, что я его понял.
Вот само задание:
Проект № 1.

Анализ публикуемых новостей

Общая задача: создать ETL-процесс формирования витрин данных для анализа публикаций новостей.

Подробное описание задачи:

Разработать скрипты загрузки данных в 2-х режимах:
o   Инициализирующий – загрузка полного слепка данных источника

o   Инкрементальный – загрузка дельты данных за прошедшие сутки

Организовать правильную структуру хранения данных
o   Сырой слой данных

o   Промежуточный слой

o   Слой витрин

В качестве результата работы программного продукта необходимо написать скрипт, который формирует витрину данных следующего содержания

Суррогатный ключ категории
Название категории
Общее количество новостей из всех источников по данной категории за все время
Количество новостей данной категории для каждого из источников за все время
Общее количество новостей из всех источников по данной категории за последние сутки
Количество новостей данной категории для каждого из источников за последние сутки
Среднее количество публикаций по данной категории в сутки
День, в который было сделано максимальное количество публикаций по данной категории
Количество публикаций новостей данной категории по дням недели
Дополнение:

Т.к. в разных источниках названия и разнообразие категорий могут отличаться, вам необходимо привести все к единому виду.

Источники:

https://lenta.ru/rss/
https://www.vedomosti.ru/rss/news
https://tass.ru/rss/v2.xml

Для решения задания я решил использовать базу даных PostgeSQL для хранения промежуточного слоя и слоя витрин.
Сырые данные сохраняются в csv файл, для каждого сайта отдельно и хранятс один день потом перезаписываются. Можно было сделать и подругому, но т.к. у меня нет жестких требования к этому слою, решил сделать именно так.
Для аркестрации данных и переодического обнавления решил использовать Аirflow + JupiterHub т.к. мы его изучали.

Для выполнения задания сначала развернул сборку на Docker (Аirflow + JupiterHub+PostgeSQL), но далее для приблежения задания к реальному решил выполнять на инфраструктуре предоставленной 1T. Так же на локальной машине использовал DBever для провеки базы данных.

